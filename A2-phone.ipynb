{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12c86dc7-29a3-4d69-8eb6-9c46c3d67192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext, datasets, math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a69c935-81eb-4d8d-aba8-6bc462d7c982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "060f1185-dd0a-400e-a3cd-a598fd00f457",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2132167/3944480506.py:13: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar.extractall(path=extracted_folder_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sw12utt',\n",
       " 'sw03utt',\n",
       " 'sw02utt',\n",
       " 'sw06utt',\n",
       " 'sw00utt',\n",
       " 'sw09utt',\n",
       " 'sw07utt',\n",
       " 'sw05utt',\n",
       " 'sw01utt',\n",
       " 'sw08utt',\n",
       " 'sw13utt',\n",
       " 'doc',\n",
       " 'README',\n",
       " 'sw10utt',\n",
       " 'sw04utt',\n",
       " 'sw11utt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "# Define the path for the uploaded file\n",
    "uploaded_file_path = 'swb1_dialogact_annot.tar.gz'\n",
    "extracted_folder_path = 'swb1_extracted/'\n",
    "\n",
    "# Create a directory for extraction if it doesn't exist\n",
    "os.makedirs(extracted_folder_path, exist_ok=True)\n",
    "\n",
    "# Extract the tar.gz file\n",
    "with tarfile.open(uploaded_file_path, \"r:gz\") as tar:\n",
    "    tar.extractall(path=extracted_folder_path)\n",
    "\n",
    "# Check the contents of the extracted folder\n",
    "extracted_files = os.listdir(extracted_folder_path)\n",
    "extracted_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "957e5369-2f4e-4bb2-a125-c76a13d65515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 261133\n",
      "First 5 rows:                                                 text\n",
      "0  *x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x...\n",
      "1  *x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x...\n",
      "2  *x*                                           ...\n",
      "3  *x*            Copyright (C) 1995 University o...\n",
      "4  *x*                                           ...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store data\n",
    "data = []\n",
    "\n",
    "# Function to process files recursively\n",
    "def process_directory(directory_path):\n",
    "    for item in os.listdir(directory_path):\n",
    "        item_path = os.path.join(directory_path, item)\n",
    "        \n",
    "        # Check if it's a file or directory\n",
    "        if os.path.isfile(item_path):  # Process only files\n",
    "            with open(item_path, 'r', encoding='utf-8') as f:\n",
    "                lines = [line.strip() for line in f.readlines() if line.strip()]  # Remove empty lines\n",
    "                data.extend(lines)\n",
    "        elif os.path.isdir(item_path):  # Recursively process directories\n",
    "            process_directory(item_path)\n",
    "\n",
    "# Start processing from the extracted folder\n",
    "process_directory(extracted_folder_path)\n",
    "\n",
    "# Convert to DataFrame\n",
    "if data:\n",
    "    df = pd.DataFrame(data, columns=[\"text\"])\n",
    "    print(f\"Dataset size: {len(df)}\")\n",
    "    print(\"First 5 rows:\", df.head())\n",
    "else:\n",
    "    print(\"No data found in the files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f6d9546-135b-4929-8d84-5c4b65b04573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from text dictionary: [(0, '*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*'), (1, '*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*'), (2, '*x*                                                                     *x*'), (3, '*x*            Copyright (C) 1995 University of Pennsylvania            *x*'), (4, '*x*                                                                     *x*')]\n"
     ]
    }
   ],
   "source": [
    "# Convert the DataFrame to a plain dictionary with only text\n",
    "text_dict = {idx: text for idx, text in enumerate(df['text'].tolist())}\n",
    "\n",
    "# Check the first few entries\n",
    "print(\"Sample from text dictionary:\", list(text_dict.items())[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca3f5251-60ff-460b-8b77-11f9ee59c2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in text_dict: dict_keys(['train', 'validation', 'test'])\n",
      "Sample train text: sd          A.81 utt4:   I have always called them, - /\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Original text list\n",
    "text_list = list(text_dict.values())  # Convert dictionary values to a list\n",
    "\n",
    "# Perform train-test-validation split\n",
    "train_texts, temp_texts = train_test_split(text_list, test_size=0.2, random_state=42)  # 80% train, 20% temp\n",
    "validation_texts, test_texts = train_test_split(temp_texts, test_size=0.5, random_state=42)  # 10% validation, 10% test\n",
    "\n",
    "# Create a new dictionary with the splits\n",
    "text_dict = {\n",
    "    'train': {idx: text for idx, text in enumerate(train_texts)},\n",
    "    'validation': {idx: text for idx, text in enumerate(validation_texts)},\n",
    "    'test': {idx: text for idx, text in enumerate(test_texts)}\n",
    "}\n",
    "\n",
    "# Verify the structure\n",
    "print(\"Keys in text_dict:\", text_dict.keys())\n",
    "print(\"Sample train text:\", text_dict['train'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97caa69c-8bc2-4d33-b5b3-01be9947cee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208906\n",
      "26113\n",
      "26114\n"
     ]
    }
   ],
   "source": [
    "# # Shuffle and then select a random sample of 1000 rows from each split\n",
    "# sampled_custom_dataset = {\n",
    "#     'train': custom_dataset['train'].shuffle(seed=42).select(range(50000)),\n",
    "#     'test': custom_dataset['test'].shuffle(seed=42).select(range(10000)),\n",
    "#     'validation': custom_dataset['validation'].shuffle(seed=42).select(range(1000)),\n",
    "# }\n",
    "\n",
    "# # Check the size of the sampled data\n",
    "# print(sampled_custom_dataset['train'])\n",
    "# print(sampled_custom_dataset['validation'])\n",
    "# print(sampled_custom_dataset['test'])\n",
    "\n",
    "print(len(train_dict))\n",
    "print(len(validation_dict))\n",
    "print(len(test_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03d532c0-8b67-4961-ae05-d9f4b081af9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First tokenized training sample: {'tokens': ['sd', 'a', '.', '81', 'utt4', 'i', 'have', 'always', 'called', 'them', ',', '-', '/']}\n",
      "First tokenized validation sample: {'tokens': ['fc', 'a', '.', '73', 'utt3', '{c', 'and', '}', '--']}\n",
      "First tokenized test sample: {'tokens': ['fp', 'a', '.', '2', 'utt1', 'hello', '.', '/']}\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_sampled_dataset = {}\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    tokenized_sampled_dataset[split] = {\n",
    "        idx: {'tokens': tokenizer(text)} for idx, text in text_dict[split].items()\n",
    "    }\n",
    "\n",
    "# Check tokenized samples\n",
    "print(\"First tokenized training sample:\", tokenized_sampled_dataset['train'][0])  # First training sample\n",
    "print(\"First tokenized validation sample:\", tokenized_sampled_dataset['validation'][0])  # First validation sample\n",
    "print(\"First tokenized test sample:\", tokenized_sampled_dataset['test'][0])  # First test sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41fea271-2a63-4602-a05b-5d360a371c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary saved to vocab2.pkl\n",
      "10531\n",
      "['<unk>', '<eos>', '.', ',', '/', 'a', 'b', '}', 'utt1', 'sd']\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import pickle\n",
    "\n",
    "# Define an iterator to extract tokens\n",
    "def yield_tokens(dataset_split):\n",
    "    for entry in dataset_split.values():  # Iterate over the dictionary values\n",
    "        yield entry['tokens']  # Extract the 'tokens' list for each entry\n",
    "\n",
    "# Build vocabulary from the iterator\n",
    "vocab = build_vocab_from_iterator(yield_tokens(tokenized_sampled_dataset['train']), min_freq=3)\n",
    "\n",
    "# Add special tokens\n",
    "vocab.insert_token('<unk>', 0)\n",
    "vocab.insert_token('<eos>', 1)\n",
    "\n",
    "# Set default index for unknown tokens\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "# Save the vocabulary\n",
    "with open(\"vocab2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vocab, f)\n",
    "\n",
    "print(\"Vocabulary saved to vocab2.pkl\")\n",
    "\n",
    "# Print information\n",
    "print(len(vocab))  # Vocabulary size\n",
    "print(vocab.get_itos()[:10])  # First 10 tokens in the vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "13bccd8f-9660-4fda-b2c0-dfa769788c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []                                                   \n",
    "    for example in dataset.values():  # Iterate over the values of the dictionary\n",
    "        if 'tokens' in example:         \n",
    "            # Append <eos> token to the sequence\n",
    "            tokens = example['tokens'] + ['<eos>']   \n",
    "            \n",
    "            # Numericalize the tokens\n",
    "            tokens = [vocab[token] for token in tokens] \n",
    "            \n",
    "            # Add tokens to the data list\n",
    "            data.extend(tokens)                                    \n",
    "    \n",
    "    # Convert to tensor\n",
    "    data = torch.LongTensor(data)\n",
    "    \n",
    "    # Make sure the data is evenly divisible by batch size\n",
    "    num_batches = data.shape[0] // batch_size\n",
    "    data = data[:num_batches * batch_size]\n",
    "    \n",
    "    # Reshape into [batch_size, num_batches]\n",
    "    data = data.view(batch_size, -1)\n",
    "    return data  # [batch_size, num_tokens_per_batch]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "576eed06-0ad1-4e5d-beee-a01d42e9f369",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_data = get_data(tokenized_sampled_dataset['train'], vocab, batch_size)\n",
    "valid_data = get_data(tokenized_sampled_dataset['validation'], vocab, batch_size)\n",
    "test_data  = get_data(tokenized_sampled_dataset['test'], vocab, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "231f9a9c-a19c-43b9-a0c3-07c6e5b3114f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
    "                \n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hid_dim = hid_dim\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, \n",
    "                    dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hid_dim, vocab_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        init_range_emb = 0.1\n",
    "        init_range_other = 1/math.sqrt(self.hid_dim)\n",
    "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n",
    "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
    "        self.fc.bias.data.zero_()\n",
    "        for i in range(self.num_layers):\n",
    "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.emb_dim,\n",
    "                    self.hid_dim).uniform_(-init_range_other, init_range_other) \n",
    "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hid_dim, \n",
    "                    self.hid_dim).uniform_(-init_range_other, init_range_other) \n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        cell   = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        return hidden, cell\n",
    "    \n",
    "    def detach_hidden(self, hidden):\n",
    "        hidden, cell = hidden\n",
    "        hidden = hidden.detach()\n",
    "        cell = cell.detach()\n",
    "        return hidden, cell\n",
    "\n",
    "    def forward(self, src, hidden):\n",
    "        #src: [batch size, seq len]\n",
    "        embedding = self.dropout(self.embedding(src))\n",
    "        #embedding: [batch size, seq len, emb_dim]\n",
    "        output, hidden = self.lstm(embedding, hidden)      \n",
    "        #output: [batch size, seq len, hid_dim]\n",
    "        #hidden = h, c = [num_layers * direction, seq len, hid_dim)\n",
    "        output = self.dropout(output) \n",
    "        prediction = self.fc(output)\n",
    "        #prediction: [batch size, seq_len, vocab size]\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb73d17c-f966-439c-8264-a74c934dfc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "507407d5-f427-406f-8782-77e182e26e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "emb_dim = 1024                # 400 in the paper\n",
    "hid_dim = 1024                # 1150 in the paper\n",
    "num_layers = 2                # 3 in the paper\n",
    "dropout_rate = 0.65              \n",
    "lr = 1e-3    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4da88b3d-a9c2-408a-876e-b3d996cc74c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 38,371,619 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3505ede8-bc00-4781-98f7-ccdab25f52cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, idx):\n",
    "    #data #[batch size, bunch of tokens]\n",
    "    src    = data[:, idx:idx+seq_len]                   \n",
    "    target = data[:, idx+1:idx+seq_len+1]  #target simply is ahead of src by 1            \n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3c77fe09-9271-4ff1-9eaf-ac6427f5cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    # drop all batches that are not a multiple of seq_len\n",
    "    # data #[batch size, bunch of tokens]\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]  #we need to -1 because we start at 0\n",
    "    num_batches = data.shape[-1]\n",
    "    \n",
    "    #reset the hidden every epoch\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    \n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #hidden does not need to be in the computational graph for efficiency\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "\n",
    "        src, target = get_batch(data, seq_len, idx) #src, target: [batch size, seq len]\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size = src.shape[0]\n",
    "        prediction, hidden = model(src, hidden)               \n",
    "\n",
    "        #need to reshape because criterion expects pred to be 2d and target to be 1d\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)  #prediction: [batch size * seq len, vocab size]  \n",
    "        target = target.reshape(-1)\n",
    "        loss = criterion(prediction, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0735bb1a-7d42-4509-9e8e-cfdf29294000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "            src, target = get_batch(data, seq_len, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            batch_size= src.shape[0]\n",
    "\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8b1342dc-090e-482a-b933-cb0371d6ad98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 39.857\n",
      "\tValid Perplexity: 61.660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 14.703\n",
      "\tValid Perplexity: 12.004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 12.800\n",
      "\tValid Perplexity: 11.245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 12.042\n",
      "\tValid Perplexity: 10.825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 11.538\n",
      "\tValid Perplexity: 10.546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 11.172\n",
      "\tValid Perplexity: 10.351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 10.881\n",
      "\tValid Perplexity: 10.205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 10.648\n",
      "\tValid Perplexity: 10.115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 10.453\n",
      "\tValid Perplexity: 10.026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 10.280\n",
      "\tValid Perplexity: 9.966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 10.124\n",
      "\tValid Perplexity: 9.917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 9.986\n",
      "\tValid Perplexity: 9.891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 9.868\n",
      "\tValid Perplexity: 9.860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 9.751\n",
      "\tValid Perplexity: 9.833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 9.648\n",
      "\tValid Perplexity: 9.823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 9.558\n",
      "\tValid Perplexity: 9.816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 9.476\n",
      "\tValid Perplexity: 9.804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 9.392\n",
      "\tValid Perplexity: 9.804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 9.200\n",
      "\tValid Perplexity: 9.779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 9.124\n",
      "\tValid Perplexity: 9.782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 9.016\n",
      "\tValid Perplexity: 9.771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 8.973\n",
      "\tValid Perplexity: 9.770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 8.917\n",
      "\tValid Perplexity: 9.770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 8.887\n",
      "\tValid Perplexity: 9.765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 8.870\n",
      "\tValid Perplexity: 9.764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 8.851\n",
      "\tValid Perplexity: 9.763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 8.843\n",
      "\tValid Perplexity: 9.762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 8.841\n",
      "\tValid Perplexity: 9.763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 8.835\n",
      "\tValid Perplexity: 9.761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 8.834\n",
      "\tValid Perplexity: 9.761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 8.837\n",
      "\tValid Perplexity: 9.761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 8.837\n",
      "\tValid Perplexity: 9.761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 8.834\n",
      "\tValid Perplexity: 9.761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 8.834\n",
      "\tValid Perplexity: 9.761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 8.830\n",
      "\tValid Perplexity: 9.761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 8.831\n",
      "\tValid Perplexity: 9.761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 8.835\n",
      "\tValid Perplexity: 9.761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 8.836\n",
      "\tValid Perplexity: 9.761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 8.838\n",
      "\tValid Perplexity: 9.761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 8.836\n",
      "\tValid Perplexity: 9.761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 8.833\n",
      "\tValid Perplexity: 9.761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 8.831\n",
      "\tValid Perplexity: 9.761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 8.831\n",
      "\tValid Perplexity: 9.761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 8.834\n",
      "\tValid Perplexity: 9.761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 8.830\n",
      "\tValid Perplexity: 9.761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 8.835\n",
      "\tValid Perplexity: 9.761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 8.832\n",
      "\tValid Perplexity: 9.761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 8.834\n",
      "\tValid Perplexity: 9.761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 8.831\n",
      "\tValid Perplexity: 9.761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 8.830\n",
      "\tValid Perplexity: 9.761\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "seq_len  = 50 #<----decoding length\n",
    "clip    = 0.25\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model, train_data, optimizer, criterion, \n",
    "                batch_size, seq_len, clip, device)\n",
    "    valid_loss = evaluate(model, valid_data, criterion, batch_size, \n",
    "                seq_len, device)\n",
    "\n",
    "    lr_scheduler.step(valid_loss)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best-val-lstm_phone_seq.pt')\n",
    "\n",
    "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d367cdab-6176-4995-99e9-e54f8e6df16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 9.803\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best-val-lstm_phone_seq.pt',  map_location=device))\n",
    "test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
    "print(f'Test Perplexity: {math.exp(test_loss):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d5c11b3f-3bb9-44cb-b088-40cab4a8c96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    model.eval()\n",
    "    tokens = tokenizer(prompt)\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            \n",
    "            #prediction: [batch size, seq len, vocab size]\n",
    "            #prediction[:, -1]: [batch size, vocab size] #probability of last vocab\n",
    "            \n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
    "            \n",
    "            while prediction == vocab['<unk>']: #if it is unk, we sample again\n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if prediction == vocab['<eos>']:    #if it is eos, we stop\n",
    "                break\n",
    "\n",
    "            indices.append(prediction) #autoregressive, thus output becomes input\n",
    "\n",
    "    itos = vocab.get_itos()\n",
    "    tokens = [itos[i] for i in indices]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9498145c-9937-4846-aae6-9ad2b7aaa8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "how are you going to be able to do that . /\n",
      "\n",
      "0.7\n",
      "how are their , -/\n",
      "\n",
      "0.75\n",
      "how are their into the problem . /\n",
      "\n",
      "0.8\n",
      "how are their into the problem . /\n",
      "\n",
      "1.0\n",
      "how are their into the album . /\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'How are '\n",
    "max_seq_len = 30\n",
    "seed = 0\n",
    "\n",
    "#smaller the temperature, more diverse tokens but comes \n",
    "#with a tradeoff of less-make-sense sentence\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n",
    "                          vocab, device, seed)\n",
    "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fa813b7b-bb1a-4b9b-b434-cb24c44d2551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "where are you going to be able to buy them . /\n",
      "\n",
      "0.7\n",
      "where are their , {f uh , } {f uh , } student and the other . /\n",
      "\n",
      "0.75\n",
      "where are their part , /\n",
      "\n",
      "0.8\n",
      "where are their into the problem you can do . /\n",
      "\n",
      "1.0\n",
      "where are their into the problem you put on and those . /\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Where are '\n",
    "max_seq_len = 30\n",
    "seed = 0\n",
    "\n",
    "#smaller the temperature, more diverse tokens but comes \n",
    "#with a tradeoff of less-make-sense sentence\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n",
    "                          vocab, device, seed)\n",
    "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fb7d0290-0e57-4d75-8545-70efa4a3f530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n",
      "the country is going to be a problem . /\n",
      "\n",
      "0.5\n",
      "the country is going to be a lot of money . /\n",
      "\n",
      "0.7\n",
      "the country is going to be a lot of money . /\n",
      "\n",
      "0.75\n",
      "the country is going into the hospital you can do . /\n",
      "\n",
      "0.8\n",
      "the country is going into the hospital you can do . /\n",
      "\n",
      "1.0\n",
      "the country is going into the hospital you put it within those once . /\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'The country '\n",
    "max_seq_len = 30\n",
    "seed = 0\n",
    "\n",
    "#smaller the temperature, more diverse tokens but comes \n",
    "#with a tradeoff of less-make-sense sentence\n",
    "temperatures = [0.2, 0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n",
    "                          vocab, device, seed)\n",
    "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3755bfee-05e5-40eb-a39f-4d871f9fcd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n",
      "she was in the middle of the house . /\n",
      "\n",
      "0.5\n",
      "she was in , {f uh , } the league of the city , /\n",
      "\n",
      "0.7\n",
      "she was in , {f uh , } ten years old . /\n",
      "\n",
      "0.75\n",
      "she was in , {f uh , } ten years old . /\n",
      "\n",
      "0.8\n",
      "she was in their own state /\n",
      "\n",
      "1.0\n",
      "she was in their own state /\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'She was in '\n",
    "max_seq_len = 30\n",
    "seed = 0\n",
    "\n",
    "#smaller the temperature, more diverse tokens but comes \n",
    "#with a tradeoff of less-make-sense sentence\n",
    "temperatures = [0.2, 0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n",
    "                          vocab, device, seed)\n",
    "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186de856-aec5-405a-9140-02040c231dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
