{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f948fb6c-d83c-4366-bb0f-eb8af6d91304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5639263a-0f75-466b-b37c-7615f7d32327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchtext\n",
      "  Downloading torchtext-0.18.0-cp312-cp312-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: tqdm in /opt/tljh/user/lib/python3.12/site-packages (from torchtext) (4.66.5)\n",
      "Requirement already satisfied: requests in /opt/tljh/user/lib/python3.12/site-packages (from torchtext) (2.32.3)\n",
      "Requirement already satisfied: torch>=2.3.0 in ./.local/lib/python3.12/site-packages (from torchtext) (2.5.1)\n",
      "Requirement already satisfied: numpy in /opt/tljh/user/lib/python3.12/site-packages (from torchtext) (2.2.0)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/tljh/user/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/tljh/user/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.local/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.local/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.local/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.local/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.local/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.local/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.local/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.local/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.local/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.local/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.local/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in ./.local/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /opt/tljh/user/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (74.1.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.local/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.3.0->torchtext) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/tljh/user/lib/python3.12/site-packages (from requests->torchtext) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/tljh/user/lib/python3.12/site-packages (from requests->torchtext) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/tljh/user/lib/python3.12/site-packages (from requests->torchtext) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/tljh/user/lib/python3.12/site-packages (from requests->torchtext) (2024.12.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/tljh/user/lib/python3.12/site-packages (from jinja2->torch>=2.3.0->torchtext) (3.0.2)\n",
      "Downloading torchtext-0.18.0-cp312-cp312-manylinux1_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Installing collected packages: torchtext\n",
      "Successfully installed torchtext-0.18.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2de41c7e-c62b-4e43-a67e-29ff710c60d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.2.0\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3\n",
      "Location: /home/jupyter-st125404/.local/lib/python3.12/site-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, nvidia-cublas-cu12, nvidia-cuda-cupti-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-runtime-cu12, nvidia-cudnn-cu12, nvidia-cufft-cu12, nvidia-curand-cu12, nvidia-cusolver-cu12, nvidia-cusparse-cu12, nvidia-nccl-cu12, nvidia-nvtx-cu12, sympy, triton, typing-extensions\n",
      "Required-by: torchtext\n",
      "---\n",
      "Name: torchtext\n",
      "Version: 0.18.0\n",
      "Summary: Text utilities, models, transforms, and datasets for PyTorch.\n",
      "Home-page: https://github.com/pytorch/text\n",
      "Author: PyTorch Text Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD\n",
      "Location: /home/jupyter-st125404/.local/lib/python3.12/site-packages\n",
      "Requires: numpy, requests, torch, tqdm\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show torch torchtext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "622910bb-a7aa-40db-94f8-d5c43c477a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torchtext==0.20.0 (from versions: 0.1.1, 0.2.0, 0.2.1, 0.2.3, 0.3.1, 0.4.0, 0.5.0, 0.6.0, 0.16.2, 0.17.2, 0.18.0)\u001b[0m\u001b[31m\n",
      "\u001b[31mERROR: No matching distribution found for torchtext==0.20.0\u001b[0m\u001b[31m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchtext==0.20.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04d54f37-5726-4d46-88ab-87b181164006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch==2.3.0\n",
      "  Downloading torch-2.3.0-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.12/site-packages (from torch==2.3.0) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/tljh/user/lib/python3.12/site-packages (from torch==2.3.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./.local/lib/python3.12/site-packages (from torch==2.3.0) (1.13.1)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.12/site-packages (from torch==2.3.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/tljh/user/lib/python3.12/site-packages (from torch==2.3.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.12/site-packages (from torch==2.3.0) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.12/site-packages (from torch==2.3.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.12/site-packages (from torch==2.3.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.12/site-packages (from torch==2.3.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.local/lib/python3.12/site-packages (from torch==2.3.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.12/site-packages (from torch==2.3.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.12/site-packages (from torch==2.3.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.12/site-packages (from torch==2.3.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.12/site-packages (from torch==2.3.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.12/site-packages (from torch==2.3.0) (12.1.0.106)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.12/site-packages (from torch==2.3.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0) (12.4.127)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/tljh/user/lib/python3.12/site-packages (from jinja2->torch==2.3.0) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.12/site-packages (from sympy->torch==2.3.0) (1.3.0)\n",
      "Downloading torch-2.3.0-cp312-cp312-manylinux1_x86_64.whl (779.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0mm\n",
      "Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Installing collected packages: nvidia-nccl-cu12, torch\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.19.3\n",
      "    Uninstalling nvidia-nccl-cu12-2.19.3:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.19.3\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.0\n",
      "    Uninstalling torch-2.2.0:\n",
      "      Successfully uninstalled torch-2.2.0\n",
      "\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx, convert-onnx-to-caffe2 and torchrun are installed in '/home/jupyter-st125404/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "Successfully installed nvidia-nccl-cu12-2.20.5 torch-2.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch==2.3.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19823159-08d9-47cd-bcb3-c3e1a0b89ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: 2: No such file or directory\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy<2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09628847-d2ad-46cc-82a4-e16be4aace10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.12/site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/tljh/user/lib/python3.12/site-packages (from datasets) (2.2.0)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-19.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /opt/tljh/user/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/tljh/user/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/tljh/user/lib/python3.12/site-packages (from datasets) (4.66.5)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting huggingface-hub>=0.23.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /opt/tljh/user/lib/python3.12/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/tljh/user/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/tljh/user/lib/python3.12/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.18.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/tljh/user/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/tljh/user/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/tljh/user/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/tljh/user/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/tljh/user/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/tljh/user/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/tljh/user/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/tljh/user/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/tljh/user/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Downloading aiohttp-3.11.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading pyarrow-19.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (283 kB)\n",
      "Downloading multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n",
      "Downloading propcache-0.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (243 kB)\n",
      "Downloading yarl-1.18.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (336 kB)\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multidict, fsspec, frozenlist, dill, aiohappyeyeballs, yarl, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.12.0\n",
      "    Uninstalling fsspec-2024.12.0:\n",
      "      Successfully uninstalled fsspec-2024.12.0\n",
      "\u001b[33m  WARNING: The script huggingface-cli is installed in '/home/jupyter-st125404/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[33m  WARNING: The script datasets-cli is installed in '/home/jupyter-st125404/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 datasets-3.2.0 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.9.0 huggingface-hub-0.27.1 multidict-6.1.0 multiprocess-0.70.16 propcache-0.2.1 pyarrow-19.0.0 xxhash-3.5.0 yarl-1.18.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12c86dc7-29a3-4d69-8eb6-9c46c3d67192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext, datasets, math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a69c935-81eb-4d8d-aba8-6bc462d7c982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f6d9546-135b-4929-8d84-5c4b65b04573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 74004228\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "# os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "\n",
    "#there are raw and preprocessed version; we used the raw one and preprocessed ourselves for fun\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the raw version of BookCorpus\n",
    "dataset = load_dataset('bookcorpus')\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca3f5251-60ff-460b-8b77-11f9ee59c2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 59203382\n",
      "}), 'validation': Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 7400423\n",
      "}), 'test': Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 7400423\n",
      "})}\n"
     ]
    }
   ],
   "source": [
    "# Split the train dataset into train, validation, and test\n",
    "train_val_test = dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
    "val_test = train_val_test['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "# Merge splits into a DatasetDict\n",
    "custom_dataset = {\n",
    "    'train': train_val_test['train'],\n",
    "    'validation': val_test['train'],\n",
    "    'test': val_test['test'],\n",
    "}\n",
    "\n",
    "print(custom_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97caa69c-8bc2-4d33-b5b3-01be9947cee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 50000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 10000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Shuffle and then select a random sample of 1000 rows from each split\n",
    "sampled_custom_dataset = {\n",
    "    'train': custom_dataset['train'].shuffle(seed=42).select(range(50000)),\n",
    "    'test': custom_dataset['test'].shuffle(seed=42).select(range(10000)),\n",
    "    'validation': custom_dataset['validation'].shuffle(seed=42).select(range(1000)),\n",
    "}\n",
    "\n",
    "# Check the size of the sampled data\n",
    "print(sampled_custom_dataset['train'])\n",
    "print(sampled_custom_dataset['validation'])\n",
    "print(sampled_custom_dataset['test'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "604b555e-daae-4eb0-921b-78c1d71e4208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what she was n't asking was if john was actually keeping an eye on mason between playing on his phone .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nIf you try to change the index you might notice that sometimes there is no paragraph \\nand rather an empty string so we will have to care of that later.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset['train'][333]['text'])\n",
    "\n",
    "'''\n",
    "If you try to change the index you might notice that sometimes there is no paragraph \n",
    "and rather an empty string so we will have to care of that later.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03d532c0-8b67-4961-ae05-d9f4b081af9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125404/.local/lib/python3.12/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be594a6b73a949b29a5649b302dbbddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd97426a237f467cb0e48bac92835120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb215ba994f4f089b9c09ff41f8a9f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['``', 'you', 'have', 'to', 'have', 'parents', ',', 'do', 'n', \"'\", 't', 'you', '?']}\n",
      "{'tokens': ['someone', 'was', 'calling', 'his', 'name', '.']}\n",
      "{'tokens': ['meaning', ',', 'does', 'she', 'have', 'a', 'love', 'of', 'green', 'day', 'starting', 'with', 'their', 'early', 'album', 'dookie', ',', 'or', 'is', 'she', 'a', 'listener', 'who', 'discovered', 'them', 'only', 'after', 'the', 'twelve-year-old-girl', 'set', 'embraced', '``', 'boulevard', 'of', 'broken', 'dreams', \"'\", \"'\", '?']}\n"
     ]
    }
   ],
   "source": [
    "# from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "# # Initialize the tokenizer\n",
    "# tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# # Function to tokenize\n",
    "# tokenize_data = lambda example, tokenizer: {'tokens': tokenizer(example['text'])}\n",
    "\n",
    "# # Map the function to each example\n",
    "# tokenized_dataset = dataset.map(tokenize_data, remove_columns=['text'], fn_kwargs={'tokenizer': tokenizer})\n",
    "\n",
    "# print(tokenized_dataset['train'][333]['tokens'])\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# Tokenize sampled datasets\n",
    "tokenized_sampled_dataset = {}\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    tokenized_sampled_dataset[split] = sampled_custom_dataset[split].map(\n",
    "        lambda example: {'tokens': tokenizer(example['text'])},\n",
    "        remove_columns=['text']\n",
    "    )\n",
    "\n",
    "# Check tokenized samples\n",
    "print(tokenized_sampled_dataset['train'][0])  # First sample\n",
    "print(tokenized_sampled_dataset['validation'][0])  # First validation sample\n",
    "print(tokenized_sampled_dataset['test'][0])  # First test sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41fea271-2a63-4602-a05b-5d360a371c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125404/.local/lib/python3.12/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/jupyter-st125404/.local/lib/python3.12/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10727\n",
      "['<unk>', '<eos>', '.', \"'\", ',', 'the', 'to', 'i', 'and', 'a']\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# Build vocabulary from the iterator\n",
    "vocab = build_vocab_from_iterator(tokenized_sampled_dataset['train']['tokens'], min_freq=3)\n",
    "\n",
    "# Add special tokens\n",
    "vocab.insert_token('<unk>', 0)\n",
    "vocab.insert_token('<eos>', 1)\n",
    "\n",
    "# Set default index for unknown tokens\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "# Print information\n",
    "print(len(vocab))  # Vocabulary size\n",
    "print(vocab.get_itos()[:10])  # First 10 tokens in the vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13bccd8f-9660-4fda-b2c0-dfa769788c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []                                                   \n",
    "    for example in dataset:\n",
    "        if example['tokens']:         \n",
    "            #appends eos so we know it ends....so model learn how to end...                             \n",
    "            tokens = example['tokens'].append('<eos>')   \n",
    "            #numericalize          \n",
    "            tokens = [vocab[token] for token in example['tokens']] \n",
    "            data.extend(tokens)                                    \n",
    "    data = torch.LongTensor(data)                                 \n",
    "    num_batches = data.shape[0] // batch_size #get the int number of batches...\n",
    "    data = data[:num_batches * batch_size] #make the batch evenly, and cut out any remaining                      \n",
    "    data = data.view(batch_size, num_batches)          \n",
    "    return data #[batch size, bunch of tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "576eed06-0ad1-4e5d-beee-a01d42e9f369",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_data = get_data(tokenized_sampled_dataset['train'], vocab, batch_size)\n",
    "valid_data = get_data(tokenized_sampled_dataset['validation'], vocab, batch_size)\n",
    "test_data  = get_data(tokenized_sampled_dataset['test'], vocab, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "231f9a9c-a19c-43b9-a0c3-07c6e5b3114f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
    "                \n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hid_dim = hid_dim\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, \n",
    "                    dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hid_dim, vocab_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        init_range_emb = 0.1\n",
    "        init_range_other = 1/math.sqrt(self.hid_dim)\n",
    "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n",
    "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
    "        self.fc.bias.data.zero_()\n",
    "        for i in range(self.num_layers):\n",
    "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.emb_dim,\n",
    "                    self.hid_dim).uniform_(-init_range_other, init_range_other) \n",
    "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hid_dim, \n",
    "                    self.hid_dim).uniform_(-init_range_other, init_range_other) \n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        cell   = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        return hidden, cell\n",
    "    \n",
    "    def detach_hidden(self, hidden):\n",
    "        hidden, cell = hidden\n",
    "        hidden = hidden.detach()\n",
    "        cell = cell.detach()\n",
    "        return hidden, cell\n",
    "\n",
    "    def forward(self, src, hidden):\n",
    "        #src: [batch size, seq len]\n",
    "        embedding = self.dropout(self.embedding(src))\n",
    "        #embedding: [batch size, seq len, emb_dim]\n",
    "        output, hidden = self.lstm(embedding, hidden)      \n",
    "        #output: [batch size, seq len, hid_dim]\n",
    "        #hidden = h, c = [num_layers * direction, seq len, hid_dim)\n",
    "        output = self.dropout(output) \n",
    "        prediction = self.fc(output)\n",
    "        #prediction: [batch size, seq_len, vocab size]\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb73d17c-f966-439c-8264-a74c934dfc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "507407d5-f427-406f-8782-77e182e26e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "emb_dim = 1024                # 400 in the paper\n",
    "hid_dim = 1024                # 1150 in the paper\n",
    "num_layers = 2                # 3 in the paper\n",
    "dropout_rate = 0.65              \n",
    "lr = 1e-3    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4da88b3d-a9c2-408a-876e-b3d996cc74c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 38,773,223 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3505ede8-bc00-4781-98f7-ccdab25f52cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, idx):\n",
    "    #data #[batch size, bunch of tokens]\n",
    "    src    = data[:, idx:idx+seq_len]                   \n",
    "    target = data[:, idx+1:idx+seq_len+1]  #target simply is ahead of src by 1            \n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c77fe09-9271-4ff1-9eaf-ac6427f5cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    # drop all batches that are not a multiple of seq_len\n",
    "    # data #[batch size, bunch of tokens]\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]  #we need to -1 because we start at 0\n",
    "    num_batches = data.shape[-1]\n",
    "    \n",
    "    #reset the hidden every epoch\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    \n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #hidden does not need to be in the computational graph for efficiency\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "\n",
    "        src, target = get_batch(data, seq_len, idx) #src, target: [batch size, seq len]\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size = src.shape[0]\n",
    "        prediction, hidden = model(src, hidden)               \n",
    "\n",
    "        #need to reshape because criterion expects pred to be 2d and target to be 1d\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)  #prediction: [batch size * seq len, vocab size]  \n",
    "        target = target.reshape(-1)\n",
    "        loss = criterion(prediction, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0735bb1a-7d42-4509-9e8e-cfdf29294000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "            src, target = get_batch(data, seq_len, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            batch_size= src.shape[0]\n",
    "\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b1342dc-090e-482a-b933-cb0371d6ad98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 359.727\n",
      "\tValid Perplexity: 215.350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 160.846\n",
      "\tValid Perplexity: 103.614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 111.975\n",
      "\tValid Perplexity: 82.925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 94.305\n",
      "\tValid Perplexity: 75.038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 85.142\n",
      "\tValid Perplexity: 70.499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 78.972\n",
      "\tValid Perplexity: 67.668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 74.260\n",
      "\tValid Perplexity: 65.400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 70.505\n",
      "\tValid Perplexity: 63.914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 67.365\n",
      "\tValid Perplexity: 62.656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 64.521\n",
      "\tValid Perplexity: 62.046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 62.151\n",
      "\tValid Perplexity: 61.623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 60.012\n",
      "\tValid Perplexity: 60.799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 58.090\n",
      "\tValid Perplexity: 60.430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 56.370\n",
      "\tValid Perplexity: 60.137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 54.789\n",
      "\tValid Perplexity: 60.201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 52.676\n",
      "\tValid Perplexity: 59.607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 51.687\n",
      "\tValid Perplexity: 59.569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 50.748\n",
      "\tValid Perplexity: 59.441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 49.915\n",
      "\tValid Perplexity: 59.494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 48.870\n",
      "\tValid Perplexity: 59.374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 48.238\n",
      "\tValid Perplexity: 59.327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 47.901\n",
      "\tValid Perplexity: 59.438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 47.284\n",
      "\tValid Perplexity: 59.335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 46.806\n",
      "\tValid Perplexity: 59.327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 46.610\n",
      "\tValid Perplexity: 59.326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 46.529\n",
      "\tValid Perplexity: 59.352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 46.465\n",
      "\tValid Perplexity: 59.370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 46.422\n",
      "\tValid Perplexity: 59.376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 46.449\n",
      "\tValid Perplexity: 59.373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 46.399\n",
      "\tValid Perplexity: 59.374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 46.422\n",
      "\tValid Perplexity: 59.373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 46.492\n",
      "\tValid Perplexity: 59.372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 46.378\n",
      "\tValid Perplexity: 59.372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 46.430\n",
      "\tValid Perplexity: 59.372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 46.382\n",
      "\tValid Perplexity: 59.372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 46.432\n",
      "\tValid Perplexity: 59.372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 46.480\n",
      "\tValid Perplexity: 59.372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 46.457\n",
      "\tValid Perplexity: 59.372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 46.395\n",
      "\tValid Perplexity: 59.372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 46.422\n",
      "\tValid Perplexity: 59.372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 46.424\n",
      "\tValid Perplexity: 59.372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 46.385\n",
      "\tValid Perplexity: 59.372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 46.416\n",
      "\tValid Perplexity: 59.372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 46.414\n",
      "\tValid Perplexity: 59.372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 46.328\n",
      "\tValid Perplexity: 59.372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 46.436\n",
      "\tValid Perplexity: 59.372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 46.422\n",
      "\tValid Perplexity: 59.372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 46.408\n",
      "\tValid Perplexity: 59.372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 46.399\n",
      "\tValid Perplexity: 59.372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 46.426\n",
      "\tValid Perplexity: 59.372\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "seq_len  = 50 #<----decoding length\n",
    "clip    = 0.25\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model, train_data, optimizer, criterion, \n",
    "                batch_size, seq_len, clip, device)\n",
    "    valid_loss = evaluate(model, valid_data, criterion, batch_size, \n",
    "                seq_len, device)\n",
    "\n",
    "    lr_scheduler.step(valid_loss)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best-val-lstm_lm.pt')\n",
    "\n",
    "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d367cdab-6176-4995-99e9-e54f8e6df16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 62.721\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best-val-lstm_lm.pt',  map_location=device))\n",
    "test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
    "print(f'Test Perplexity: {math.exp(test_loss):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5c11b3f-3bb9-44cb-b088-40cab4a8c96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    model.eval()\n",
    "    tokens = tokenizer(prompt)\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            \n",
    "            #prediction: [batch size, seq len, vocab size]\n",
    "            #prediction[:, -1]: [batch size, vocab size] #probability of last vocab\n",
    "            \n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
    "            \n",
    "            while prediction == vocab['<unk>']: #if it is unk, we sample again\n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if prediction == vocab['<eos>']:    #if it is eos, we stop\n",
    "                break\n",
    "\n",
    "            indices.append(prediction) #autoregressive, thus output becomes input\n",
    "\n",
    "    itos = vocab.get_itos()\n",
    "    tokens = [itos[i] for i in indices]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9498145c-9937-4846-aae6-9ad2b7aaa8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "i am going to do with me .\n",
      "\n",
      "0.7\n",
      "i am , ' ' he said .\n",
      "\n",
      "0.75\n",
      "i am wearing a lot of paper ?\n",
      "\n",
      "0.8\n",
      "i am wearing a date , but they have n ' t even chosen the intimate gentleman .\n",
      "\n",
      "1.0\n",
      "i am wearing a date , but they have before a things over her fellow . ' '\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'I am '\n",
    "max_seq_len = 30\n",
    "seed = 0\n",
    "\n",
    "#smaller the temperature, more diverse tokens but comes \n",
    "#with a tradeoff of less-make-sense sentence\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n",
    "                          vocab, device, seed)\n",
    "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa813b7b-bb1a-4b9b-b434-cb24c44d2551",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
